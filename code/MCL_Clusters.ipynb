{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7de4a2e-bf13-4296-a82f-a13348091d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://markov-clustering.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "991a2b9b-276c-40f5-bffb-b178af92078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "from itertools import combinations, chain\n",
    "\n",
    "from dynamicTreeCut import cutreeHybrid\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import more_itertools as mit\n",
    "import random\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import show, cm, axis\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "from collections import OrderedDict, Counter\n",
    "np.set_printoptions(edgeitems=10, linewidth=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "605ff5df-86ee-442e-bd90-c78cb21e3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCL_Cluster(object):\n",
    "    def __init__(self, merge_threshold=0.25, min_complex_size=3, max_complex_size=100, split_depth=1):\n",
    "        self.merge_threshold = merge_threshold\n",
    "        self.min_complex_size = min_complex_size\n",
    "        self.max_complex_size = max_complex_size\n",
    "        self.split_depth = split_depth\n",
    "        self.break_num = 0\n",
    "        self.merge_num = 0\n",
    "        self.max_break_num = 0\n",
    "        self.max_merge_num = 7\n",
    "    \n",
    "    def create_edge_adj(self, e_idx, e_w=None, dim=None, diag=None):\n",
    "        self.dim = dim\n",
    "        mat = np.zeros([dim, dim])\n",
    "        if e_w is not None:\n",
    "            if e_idx.shape[0]==2:\n",
    "                mat[e_idx[0], e_idx[1]] = e_w\n",
    "            elif e_idx.shape[1]==2:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = e_w\n",
    "            else:\n",
    "                print('wrong edge dimension')\n",
    "                return\n",
    "        else:\n",
    "            if e_idx.shape[0]==3:\n",
    "                mat[e_idx[0], e_idx[1]] = e_idx[2]\n",
    "            elif e_idx.shape[1]==3:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = e_idx[:,2]\n",
    "            elif e_idx.shape[0]==2:\n",
    "                mat[e_idx[0], e_idx[1]] = 1.0\n",
    "                print('Not give edge weight, use 1.0 as weight')\n",
    "            elif e_idx.shape[1]==2:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = 1.0\n",
    "                print('Not give edge weight, use 1.0 as weight')\n",
    "        mat += mat.transpose()\n",
    "        if diag is not None:\n",
    "            np.fill_diagonal(mat, diag)\n",
    "        return mat\n",
    "    \n",
    "    def create_tom_adj(self, A):\n",
    "        d = A.shape[0]\n",
    "        L = A.dot(A.T)\n",
    "        K = A.sum(axis=1)\n",
    "\n",
    "        A_tom = np.zeros_like(A)\n",
    "        for i in range(d):\n",
    "            for j in range(i+1, d):  \n",
    "                numerator = L[i, j] + A[i, j]\n",
    "                denominator = min(K[i], K[j]) + 1 - A[i, j]\n",
    "                A_tom[i, j] = numerator / denominator\n",
    "\n",
    "        A_tom += A_tom.T\n",
    "        np.fill_diagonal(A_tom, 0)\n",
    "        return np.nan_to_num(A_tom)\n",
    "    \n",
    "    def iterate(self, A, expand_power, inflate_power):\n",
    "        A_ = np.linalg.matrix_power(A, expand_power)  # expand\n",
    "        A_ = np.power(A_, inflate_power)              # inflate\n",
    "        A_ = A_ / A_.sum(0)                           # col-wise normalization\n",
    "        AA = A_.copy()\n",
    "        A_[A_ < 0.001] = 0                            # pruning\n",
    "        num_cols = AA.shape[1]\n",
    "        col_indices = np.arange(num_cols)\n",
    "        row_indices = A.argmax(axis=0).reshape((num_cols,)) # keep max of each col in original matrix\n",
    "        A_[row_indices, col_indices] = AA[row_indices, col_indices]\n",
    "        return A_\n",
    "    \n",
    "    def MCL_process(self, A, expand_power=2, inflate_power=2, iters=2):\n",
    "        A_ = A / A.sum(0)\n",
    "        for j in range(iters):\n",
    "            A_ = self.iterate(A_, expand_power, inflate_power)\n",
    "        np.fill_diagonal(A_, 0)\n",
    "        return A_\n",
    "    \n",
    "    def label_nodes(self, n_idx, lab):\n",
    "        # https://stackoverflow.com/questions/39387435/group-list-elements-based-on-another-list\n",
    "        sidx = lab.argsort()                                    # Get sidx (sorted indices) for label\n",
    "        split_idx = np.flatnonzero(np.diff(lab[sidx]) > 0) + 1  # Get where the sorted version of label changes groups\n",
    "        groups = np.split(n_idx[sidx], split_idx)               # Sort input based on the sidx and split label on split_idx\n",
    "        return [tuple(sorted(i)) for i in groups]\n",
    "    \n",
    "    def remove_subset(self, Cs):\n",
    "        return list(filter(lambda f: not any(set(f) < set(g) for g in Cs), Cs))\n",
    "    \n",
    "    def get_modularity(self, A, Cs, gamma=1.0):\n",
    "        Lc_m = [( np.array(list(combinations(Cs[j], 2))).shape[0]  / ((np.array(A.nonzero()).T.shape[0] - A.shape[0])/2) ) for j in range(len(Cs))]\n",
    "        kc_2m = [(( sum([A[k, :].sum() for k in Cs[j]]) / (np.array(A.nonzero()).T.shape[0] - A.shape[0]) )**2)*gamma for j in range(len(Cs))]\n",
    "        M = np.array(Lc_m)-np.array(kc_2m)\n",
    "        return M\n",
    "    \n",
    "    def extract_clusters(self, A, keep_idx=None):\n",
    "        print('extract clusters')\n",
    "        \n",
    "        zero_idx = np.array(list(set(np.where(A.sum(0)!=0)[0])&set(np.where(A.sum(1)!=0)[0])))\n",
    "        \n",
    "        I = self.create_tom_adj(np.power(A, 2.5))\n",
    "        \n",
    "        if keep_idx is None:\n",
    "            keep_idx = zero_idx\n",
    "        else:\n",
    "            keep_idx = np.array(list( set(keep_idx) & set(zero_idx) ))\n",
    "        \n",
    "        A_ = A[keep_idx, :][:, keep_idx]\n",
    "        A_ = self.MCL_process(A_, iters=3)\n",
    "        I_ = I[keep_idx, :][:, keep_idx]\n",
    "        \n",
    "        imp_1 = A_*0.3 + I_*0.7\n",
    "        imp_2 = A_*0.1 + I_*0.9\n",
    "        \n",
    "        dist_1 = pdist(imp_1, metric='cosine')\n",
    "        dist_2 = pdist(imp_2, metric='cosine')\n",
    "        \n",
    "        dist_1[~np.isfinite(dist_1)] = 1.0\n",
    "        dist_2[~np.isfinite(dist_2)] = 1.0\n",
    "        \n",
    "        link_1 = linkage(dist_1, 'ward')\n",
    "        link_2 = linkage(dist_2, 'ward')\n",
    "        \n",
    "        Cs_lab_1 = cutreeHybrid(link_1, dist_1, minClusterSize=(self.min_complex_size), deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "        Cs_lab_2 = cutreeHybrid(link_2, dist_2, minClusterSize=(self.min_complex_size), deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "        Cs_lab_1 = Cs_lab_1['labels']\n",
    "        Cs_lab_2 = Cs_lab_2['labels']\n",
    "        \n",
    "        Cs_1 = self.label_nodes(keep_idx, Cs_lab_1)\n",
    "        Cs_2 = self.label_nodes(keep_idx, Cs_lab_2)\n",
    "        Cs = Cs_1 + Cs_2\n",
    "        \n",
    "        # m = self.get_modularity(I, Cs, gamma=10)\n",
    "        # Cs_ = [Cs[j] for j in np.where(m <= 0.00)[0]]\n",
    "        # Cs = [Cs[j] for j in np.where(m > 0.00)[0]]\n",
    "        \n",
    "        # small_Cs = [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)] + [j for j in Cs_ if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "        small_Cs = [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "        large_Cs = [j for j in Cs if len(j) > self.max_complex_size]\n",
    "        \n",
    "        while len(large_Cs) > 0 and self.break_num <= self.max_break_num:\n",
    "            print('break clusters, break iter: ', self.break_num)\n",
    "            \n",
    "            idx = 0\n",
    "            max_idx = len(large_Cs)\n",
    "            max_idx_ori = len(large_Cs)\n",
    "            while (idx < max_idx)&(idx < max_idx_ori):\n",
    "                cur_large_C = large_Cs[idx]\n",
    "                \n",
    "                keep_idx = np.array(list( set(cur_large_C) & set(zero_idx) ))\n",
    "                # keep_idx = np.array(list( set(chain.from_iterable([j for j in cur_large_C])) & set(zero_idx) ))\n",
    "                \n",
    "                A_ = A[keep_idx, :][:, keep_idx]\n",
    "                A_ = self.MCL_process(A_, iters=5)\n",
    "                I_ = I[keep_idx, :][:, keep_idx]\n",
    "                \n",
    "                imp_1 = A_*0.3 + I_*0.7\n",
    "                imp_2 = A_*0.1 + I_*0.9\n",
    "\n",
    "                dist_1 = pdist(imp_1, metric='cosine')\n",
    "                dist_2 = pdist(imp_2, metric='cosine')\n",
    "\n",
    "                dist_1[~np.isfinite(dist_1)] = 1.0\n",
    "                dist_2[~np.isfinite(dist_2)] = 1.0\n",
    "\n",
    "                link_1 = linkage(dist_1, 'ward')\n",
    "                link_2 = linkage(dist_2, 'ward')\n",
    "\n",
    "                Cs_lab_1 = cutreeHybrid(link_1, dist_1, minClusterSize=(self.min_complex_size), deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "                Cs_lab_2 = cutreeHybrid(link_2, dist_2, minClusterSize=(self.min_complex_size), deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "                Cs_lab_1 = Cs_lab_1['labels']\n",
    "                Cs_lab_2 = Cs_lab_2['labels']\n",
    "\n",
    "                Cs_1 = self.label_nodes(keep_idx, Cs_lab_1)\n",
    "                Cs_2 = self.label_nodes(keep_idx, Cs_lab_2)\n",
    "                Cs = Cs_1 + Cs_2\n",
    "                \n",
    "                small_Cs = small_Cs + [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "                large_Cs = large_Cs + [j for j in Cs if len(j) > self.max_complex_size]\n",
    "                large_Cs = self.remove_subset(large_Cs)\n",
    "                idx += 1\n",
    "                max_idx = len(large_Cs)\n",
    "            self.break_num += 1\n",
    "        self.break_num = 0\n",
    "        # small_Cs = self.remove_subset(small_Cs)\n",
    "        return small_Cs\n",
    "    \n",
    "    def get_ove_score(self, list1, list2):\n",
    "        if type(list1[0])!=list and type(list1[0])!=tuple:\n",
    "            list1 = [list1]\n",
    "        if type(list2[0])!=list and type(list2[0])!=tuple:\n",
    "            list2 = [list2]\n",
    "        \n",
    "        s1 = np.array([[len(set(list1[k])&set(tuple(j)))**2 for j in list2 ] for k in range(len(list1))])\n",
    "        s2 = np.array([[len(list1[k])*len(j) for j in list2 ] for k in range(len(list1))])\n",
    "        ove_score = np.nan_to_num(s1 / s2)\n",
    "        if ove_score.shape[0] > 1:\n",
    "            np.fill_diagonal(ove_score, 0)\n",
    "            return ove_score\n",
    "        else:\n",
    "            return ove_score.reshape(-1)\n",
    "    \n",
    "    def merge_clusters(self, Cs, A):\n",
    "        if_merge = True\n",
    "        while if_merge:\n",
    "            print('merge clusters, merge iter: ', self.merge_num)\n",
    "            ove_score = self.get_ove_score(Cs, Cs)\n",
    "            ove_score = np.triu(ove_score, k=1) # keep only upper traingular values\n",
    "            \n",
    "            ## Combind cluster id with their overlap score\n",
    "            ove_clusters_pair_id_w = list(zip(zip(np.arange(ove_score.shape[0]), ove_score.argmax(1)), ove_score.max(1))) # [(cluster_id1, cluster_id2), ove_score]\n",
    "            ove_clusters_pair_id_w = [j for j in ove_clusters_pair_id_w if j[1] >= self.merge_threshold]                  # keep those with overlap score >= threshold\n",
    "            \n",
    "            if len(ove_clusters_pair_id_w) < 1:\n",
    "                if_merge = False\n",
    "            else:\n",
    "                ## Index of clusters with similarity highr than a threshold\n",
    "                ove_clusters_id = list(set([j for k in [i[0] for i in ove_clusters_pair_id_w] for j in k]))\n",
    "                \n",
    "                ## Sorted by overlap score\n",
    "                ove_clusters_pair_id_w = sorted(ove_clusters_pair_id_w, key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                ## Combind cluster id, overlap score, and overlap size\n",
    "                ove_clusters_pair_id_w_size = [ove_clusters_pair_id_w[idx] + \n",
    "                                               ( len(set(Cs[ove_clusters_pair_id_w[idx][0][0]] + \n",
    "                                                         Cs[ove_clusters_pair_id_w[idx][0][1]])), ) \n",
    "                                               for idx in range(len(ove_clusters_pair_id_w))] # [(cluster_id1, cluster_id2), ove_score, ove_size]\n",
    "                \n",
    "                ## filter by max size\n",
    "                ove_clusters_pair_id_w_size = [j for j in ove_clusters_pair_id_w_size if j[2] <= self.max_complex_size]\n",
    "                \n",
    "                if len(ove_clusters_pair_id_w_size) < 1:\n",
    "                    if_merge = False\n",
    "                else:\n",
    "                    ## combind pairs to set\n",
    "                    ove_clusters_pair_id_w_size_set = [[j for j in ove_clusters_pair_id_w_size if k in j[0]] for k in ove_clusters_id]\n",
    "                    ove_clusters_pair_id_w_size_set = [j for j in ove_clusters_pair_id_w_size_set if len(j) > 0]\n",
    "                    \n",
    "                    if len(ove_clusters_pair_id_w_size_set) < 1:\n",
    "                        if_merge = False\n",
    "                    else:\n",
    "                        ## Collect all the id of clusters that need to merge\n",
    "                        ove_clusters_id_set = list(set( [tuple(sorted( set(chain.from_iterable( [j[0] for j in ove_clusters_pair_id_w_size_set[k]] )) )) \n",
    "                                                         for k in range(len(ove_clusters_pair_id_w_size_set))] ))\n",
    "                        \n",
    "                        ## Remove subset\n",
    "                        ove_clusters_id_set_sub = self.remove_subset(ove_clusters_id_set)\n",
    "                        \n",
    "                        ## Index of clusters that will be merged / not be merged\n",
    "                        ove_clusters_id = list(set([k for j in ove_clusters_id_set_sub for k in j]))\n",
    "                        non_ove_clusters_id = list(set(range(len(Cs)))^set(ove_clusters_id))\n",
    "                        \n",
    "                        Cs = [tuple(sorted( set(chain.from_iterable([Cs[j] for j in ove_clusters_id_set_sub[k]])) )) \n",
    "                              for k in range(len(ove_clusters_id_set_sub))] + [Cs[j] for j in non_ove_clusters_id]\n",
    "                        \n",
    "                        small_Cs = [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "                        large_Cs = [j for j in Cs if len(j) > self.max_complex_size]\n",
    "                        \n",
    "                        if len(large_Cs) > 0 and self.merge_num <= self.max_merge_num:\n",
    "                            keep_idx = np.array(list(set(chain.from_iterable([j for j in large_Cs]))))\n",
    "                            small_Cs = small_Cs + self.extract_clusters(A, keep_idx=keep_idx)\n",
    "                            small_Cs = self.remove_subset(small_Cs)\n",
    "                        else:\n",
    "                            if_merge = False\n",
    "                        \n",
    "                        Cs = small_Cs\n",
    "                        self.merge_num += 1\n",
    "        \n",
    "        self.merge_num = 0\n",
    "        return Cs\n",
    "    \n",
    "    def run(self, e_idx, e_w=None, dim=None, diag=None):\n",
    "        if dim is None:\n",
    "            print('Dim must specified')\n",
    "            return\n",
    "        else:\n",
    "            A = self.create_edge_adj(e_idx, e_w=e_w, dim=dim, diag=diag)\n",
    "            Cs = self.extract_clusters(A)\n",
    "            Cs = self.merge_clusters(Cs, A)\n",
    "            Cs = self.remove_subset(Cs)\n",
    "            Cs = list(set(Cs))\n",
    "            return Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57dff0-7596-4b2c-901f-8ee0986cfb84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2224041d-9dbc-420e-8b96-adae71bfa1fd",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6c2f7-e8b5-43a3-8c95-e39c8a97694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting\n",
    "exp_name = 'PXD002892'\n",
    "\n",
    "exp_path = ['./output/' + i for i in os.listdir('./output/')]\n",
    "exp_path = [i for i in exp_path if ('all_emb' not in i)&('checkpoint' not in i)]\n",
    "print(*exp_path, sep='\\n')\n",
    "exp_cond = sorted(set([i.split('/')[-1].split('_')[0] for i in exp_path]))\n",
    "print(exp_cond)\n",
    "\n",
    "load_dir1 = 'path to input' + '/' + exp_name\n",
    "dict_path = ['/'.join([load_dir1, i]) for i in os.listdir(load_dir1) if 'idx_dict' in i]\n",
    "dict_path = [i for i in dict_path if 'SEC2' in i]\n",
    "dict_path = [i for i in dict_path if (re.sub('.pickle', '', i.split('/')[-1].split('_')[-1]) in exp_cond)]\n",
    "print(*dict_path, sep='\\n')\n",
    "\n",
    "load_dir2 = 'path to Split-data' + exp_name\n",
    "split_path = [load_dir2 + '/' + i for i in os.listdir(load_dir2) if ('ref' in i) or ('heldout' in i) or ('exp' in i)]\n",
    "split_path = [i for i in split_path if (re.sub('.rds', '', i.split('/')[-1].split('_')[-1]) in exp_cond)]\n",
    "print(*split_path, sep='\\n')\n",
    "\n",
    "load_dir3 = 'path to Protein complex-data'\n",
    "print(load_dir3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1f15d-b136-49c7-8f49-b2d3db055fab",
   "metadata": {},
   "source": [
    "### Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f13d84-7b87-4b09-9632-a498d606e48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ComplexName</th>\n",
       "      <th>UniProt_entry</th>\n",
       "      <th>Gene_name</th>\n",
       "      <th>Gene_name_primary</th>\n",
       "      <th>Gene_entrez</th>\n",
       "      <th>UniProt_entry_N</th>\n",
       "      <th>Gene_name_N</th>\n",
       "      <th>Gene_name_primary_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nuclear pore complex</td>\n",
       "      <td>Q9NRG9</td>\n",
       "      <td>AAAS</td>\n",
       "      <td>AAAS</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AATF-NGDN-NOL10 complex</td>\n",
       "      <td>Q9NY61</td>\n",
       "      <td>AATF</td>\n",
       "      <td>AATF</td>\n",
       "      <td>26574</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCB1-PPP2R3C-TFPI2 complex</td>\n",
       "      <td>P08183</td>\n",
       "      <td>ABCB1</td>\n",
       "      <td>ABCB1</td>\n",
       "      <td>5243</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCB1-ANXA2-RACK1-SRC complex</td>\n",
       "      <td>P08183</td>\n",
       "      <td>ABCB1</td>\n",
       "      <td>ABCB1</td>\n",
       "      <td>5243</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNW1 complex</td>\n",
       "      <td>Q9UG63</td>\n",
       "      <td>ABCF2</td>\n",
       "      <td>ABCF2</td>\n",
       "      <td>10061</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12686</th>\n",
       "      <td>EMX1-GLI3-WDR11 complex</td>\n",
       "      <td>Q8WWQ0</td>\n",
       "      <td>WDR11</td>\n",
       "      <td>PHIP</td>\n",
       "      <td>55717</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12687</th>\n",
       "      <td>CEN complex</td>\n",
       "      <td>Q8WWQ0</td>\n",
       "      <td>WDR11</td>\n",
       "      <td>PHIP</td>\n",
       "      <td>55717</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12688</th>\n",
       "      <td>TNFR2 signaling complex</td>\n",
       "      <td>Q9NQH7</td>\n",
       "      <td>XPNPEP3</td>\n",
       "      <td>XPNPEP3</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12689</th>\n",
       "      <td>AIM2 PANoptosome complex</td>\n",
       "      <td>Q9NZI8</td>\n",
       "      <td>ZBP1</td>\n",
       "      <td>IGF2BP1</td>\n",
       "      <td>81030</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>PANoptosome</td>\n",
       "      <td>Q9NZI8</td>\n",
       "      <td>ZBP1</td>\n",
       "      <td>IGF2BP1</td>\n",
       "      <td>81030</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12691 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ComplexName UniProt_entry Gene_name  \\\n",
       "0               Nuclear pore complex        Q9NRG9      AAAS   \n",
       "1            AATF-NGDN-NOL10 complex        Q9NY61      AATF   \n",
       "2        ABCB1-PPP2R3C-TFPI2 complex        P08183     ABCB1   \n",
       "3      ABCB1-ANXA2-RACK1-SRC complex        P08183     ABCB1   \n",
       "4                       SNW1 complex        Q9UG63     ABCF2   \n",
       "...                              ...           ...       ...   \n",
       "12686        EMX1-GLI3-WDR11 complex        Q8WWQ0     WDR11   \n",
       "12687                    CEN complex        Q8WWQ0     WDR11   \n",
       "12688        TNFR2 signaling complex        Q9NQH7   XPNPEP3   \n",
       "12689       AIM2 PANoptosome complex        Q9NZI8      ZBP1   \n",
       "12690                    PANoptosome        Q9NZI8      ZBP1   \n",
       "\n",
       "      Gene_name_primary Gene_entrez  UniProt_entry_N  Gene_name_N  \\\n",
       "0                  AAAS        None               31           31   \n",
       "1                  AATF       26574                3            3   \n",
       "2                 ABCB1        5243                3            3   \n",
       "3                 ABCB1        5243                4            4   \n",
       "4                 ABCF2       10061               18           18   \n",
       "...                 ...         ...              ...          ...   \n",
       "12686              PHIP       55717                4            3   \n",
       "12687              PHIP       55717               38           37   \n",
       "12688           XPNPEP3        None                4            3   \n",
       "12689           IGF2BP1       81030               10            9   \n",
       "12690           IGF2BP1       81030                8            7   \n",
       "\n",
       "       Gene_name_primary_N  \n",
       "0                       31  \n",
       "1                        3  \n",
       "2                        3  \n",
       "3                        4  \n",
       "4                       18  \n",
       "...                    ...  \n",
       "12686                    3  \n",
       "12687                   38  \n",
       "12688                    3  \n",
       "12689                   10  \n",
       "12690                    8  \n",
       "\n",
       "[12691 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = list(pyreadr.read_r(load_dir3 + '/Complexes_gene_Human_filter.rds').values())[0]\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abac648-6037-4d06-aaea-f382edfa2cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2277\n",
      "['SNRPE', 'SNRPF', 'SNRPG']\n",
      "['PDCD7', 'PRKRIP1', 'SNRNP25', 'SNRNP35', 'SNRNP48', 'SNRPB', 'SNRPD1', 'SNRPD2', 'SNRPD3', 'SNRPE', 'SNRPF', 'SNRPG', 'SRSF7', 'TOE1', 'YBX1', 'SNRPD1', 'SNRPD1']\n",
      "['MAPT', 'SGK1', 'YWHAQ']\n",
      "['CHERP', 'DDX46', 'DHX15', 'DNAJC8', 'HMG20B', 'HSPD1', 'PHF5A', 'PUF60', 'RBM17', 'SF3A1', 'SF3A2', 'SF3A3', 'SF3B1', 'SF3B2', 'SF3B3', 'SF3B4', 'SF3B5', 'SF3B6', 'SMNDC1', 'SNRPA1', 'SNRPB', 'SNRPB2', 'SNRPD1', 'SNRPD2', 'SNRPD3', 'SNRPE', 'SNRPF', 'SNRPG', 'SRSF1', 'TRAP1', 'U2AF1', 'U2AF2', 'U2SURP', 'SNRPD1', 'TRAP1', 'SNRPD1', 'TRAP1']\n",
      "['DHX15', 'PDCD7', 'PRPF8', 'RNPC3', 'SF3B1', 'SF3B2', 'SF3B3', 'SF3B4', 'SF3B5', 'SF3B6', 'SNRNP25', 'SNRNP35', 'SNRNP48', 'SNRPB', 'SNRPD1', 'SNRPD2', 'SNRPD3', 'SNRPE', 'SNRPF', 'SNRPG', 'YBX1', 'ZCRB1', 'ZMAT5', 'ZRSR2', 'SNRPD1', 'SNRPD1']\n"
     ]
    }
   ],
   "source": [
    "gs = list(gs.groupby('ComplexName')['Gene_name'].apply(list).values)\n",
    "print(len(gs))\n",
    "print(*gs[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b370ba-91cd-4511-af7e-4704193ccf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[4002]\n"
     ]
    }
   ],
   "source": [
    "name_idx_dict = []\n",
    "for cur_dict_path in dict_path:\n",
    "    with open(cur_dict_path, 'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "    name_idx_dict.append(temp)\n",
    "print(len(name_idx_dict))\n",
    "print([len(name_idx_dict[i]) for i in range(len(name_idx_dict))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877f501f-df20-44b3-a1b1-53c4c848a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[2277]\n"
     ]
    }
   ],
   "source": [
    "### filter gs by exp name\n",
    "gs_sub = [[[i for i in gs[j] if i in name_idx_dict[k].keys()] for j in range(len(gs))] for k in range(len(name_idx_dict))]\n",
    "print(len(gs_sub))\n",
    "print([len(gs_sub[i]) for i in range(len(gs_sub))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac0b2b3-b782-41e0-b18b-9ed48012018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[2277]\n",
      "(1437, 1438, 1439)\n",
      "(1433, 1434, 1435, 1436, 1437, 1438, 1439, 1467, 1554, 1688, 1434, 1434)\n",
      "(742, 1695)\n",
      "(218, 332, 339, 565, 602, 1030, 1182, 1225, 1383, 1384, 1385, 1386, 1387, 1388, 1423, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1460, 1568, 1607, 1608, 1609, 1434, 1568, 1434, 1568)\n",
      "(339, 1132, 1385, 1386, 1387, 1388, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1688, 1434, 1434)\n"
     ]
    }
   ],
   "source": [
    "### match name by idx\n",
    "complexes = [[tuple([name_idx_dict[k][i] for i in gs_sub[k][j]]) for j in range(len(gs_sub[k]))] for k in range(len(name_idx_dict))]\n",
    "print(len(complexes))\n",
    "print([len(complexes[i]) for i in range(len(complexes))])\n",
    "print(*complexes[0][:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635e9da9-4be0-4345-a8e8-2d13a415e637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[941]\n",
      "(1437, 1438, 1439)\n",
      "(1433, 1434, 1435, 1436, 1437, 1438, 1439, 1467, 1554, 1688, 1434, 1434)\n",
      "(218, 332, 339, 565, 602, 1030, 1182, 1225, 1383, 1384, 1385, 1386, 1387, 1388, 1423, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1460, 1568, 1607, 1608, 1609, 1434, 1568, 1434, 1568)\n",
      "(339, 1132, 1385, 1386, 1387, 1388, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1688, 1434, 1434)\n",
      "(233, 1123, 1433, 1434, 1435, 1436, 1434, 1434)\n"
     ]
    }
   ],
   "source": [
    "### filter by complex size\n",
    "complexes = [[i for i in complexes[j] if len(i) > 2] for j in range(len(complexes))]\n",
    "print(len(complexes))\n",
    "print([len(complexes[i]) for i in range(len(complexes))])\n",
    "print(*complexes[0][:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b3762-40e6-4410-ae94-655f6e633e61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9e41b95-b63d-4602-a15d-b3996edf77eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "edges = deepcopy(exp_cond)\n",
    "w = deepcopy(exp_cond)\n",
    "n_nodes = deepcopy(exp_cond)\n",
    "for l in range(len(exp_cond)):\n",
    "    cur_split_path = [split_path[j] for j in [i for i, v in enumerate(split_path) if exp_cond[l] in v]]\n",
    "    cur_df = list(pyreadr.read_r([i for i in cur_split_path if 'ref' in i][0]).values())[0]\n",
    "    cur_df = pd.concat([cur_df, list(pyreadr.read_r([i for i in cur_split_path if 'heldout' in i][0]).values())[0]], ignore_index=True, sort=False)\n",
    "    cur_df = pd.concat([cur_df, list(pyreadr.read_r([i for i in cur_split_path if 'exp' in i][0]).values())[0]], ignore_index=True, sort=False)\n",
    "\n",
    "    cur_cv_path = [i for i in exp_path if exp_cond[l] in i]\n",
    "    \n",
    "    cur_df['cv1'] = np.concatenate([np.load([i for i in cur_cv_path if ('train' in i)][0])['arr_0'], \n",
    "                                    np.load([i for i in cur_cv_path if ('heldout' in i)][0])['arr_0'], \n",
    "                                    np.load([i for i in cur_cv_path if ('delfold' in i)][0])['arr_0'],\n",
    "                                    np.load([i for i in cur_cv_path if ('exp' in i)][0])['arr_0']], 0)\n",
    "\n",
    "    cur_edge = cur_df.iloc[:, 0:2].values\n",
    "    cur_label = np.mean(cur_df.loc[:, [col for col in cur_df.columns if 'cv' in col]].values, 1)\n",
    "    cur_edge = np.array(list(zip([name_idx_dict[l][i] for i in cur_edge[:, 0]], [name_idx_dict[l][i] for i in cur_edge[:, 1]])))\n",
    "    \n",
    "    edges[l] = cur_edge\n",
    "    w[l] = cur_label \n",
    "    n_nodes[l] = list(pyreadr.read_r( '/'.join(['path to EPF-data', epx_name, exp_cond[l]+'.rds']) ).values())[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3711fc-3629-4e80-97d9-1e2a9da92617",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec500912-f92d-4a81-a43f-70c368965a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract clusters\n",
      "..cutHeight not given, setting it to 18.43027583126573  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 18.42918914652604  ===>  99% of the (truncated) height range in dendro.\n",
      "break clusters, break iter:  0\n",
      "..cutHeight not given, setting it to 0.6323751461903582  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6324066455765278  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.9559233199587112  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.9560512508285544  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.7274468547297068  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.7276258147156645  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6482487522355823  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6256272315041701  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.8347236929618536  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.8347436819716377  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.219181322668719  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.23286569385689843  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 2.489403751587785  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 2.5149327277625133  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.08249602785862767  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.08282831918392392  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6323751461903582  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6324066455765278  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.9559233199587112  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.9560512508285544  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.7274468547297068  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.7276258147156645  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6482487522355823  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.6256272315041701  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 1.8919151208681046  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 1.89211224659575  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.219181322668719  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.23286569385689843  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 2.489403751587785  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 2.5149327277625133  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.08249602785862767  ===>  99% of the (truncated) height range in dendro.\n",
      "..cutHeight not given, setting it to 0.08282831918392392  ===>  99% of the (truncated) height range in dendro.\n",
      "merge clusters, merge iter:  0\n",
      "PXD002892 SEC2-heavy\n",
      "127 3 100\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARgAAAEYCAYAAACHjumMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASbElEQVR4nO3df7Ddd13n8efLFkQKpYm9iWmLBHcjiN1tcS7YblkXG4otIMmMdi1unSvbmczs4koFf6SiO4szuxMdBvEnu1lA7kot1lJMLApkoh1/ldYbqdia1ggNbWlIbqvQUkVofe8f55NyCDe95+beT+49t8/HzJnv+X6+v97nzD2v+/n+OOebqkKSevi65S5A0uplwEjqxoCR1I0BI6kbA0ZSNwaMpG4MmDGSpJI8muR/Lnct80nyP5K8b7nrOJ4kVyX5QntP//Vy17NaGTDj57yqegtAko1JDh6dkORlSf48yeeT/H2SP0vykjbth5M83j5Uw4+zhpb/wSQzrf1Qkj9I8rI2bSrJviQPJ7k/yS8kOfUkv/ZFSXJzkpcDVNW7q+pZy1vR6mfArBJJTgduAn4FWAucDbwV+Oeh2W6pqmcd83igLf8m4B3A/wLWA98M/DqwpS37TOBq4EzgO4HNwI93flkacwbM6vGtAFV1XVU9XlX/VFUfrapPzLdgkucAPwe8oapurKpHq+rLVfV7VfUTbb3vrKo/qaovVdVngGuBi+ZZ9dOT/L8kjyS5M8nk0DbPSvKBJLNJ7knyo0PTXprkliSfaz2pX03y9Dbtfyd52zH170rypiQ/keQDx0z7lSTvmO89UB8GzBirqoNVtbGN/i3weJLpJJclWbOAVV0IPAP44AKW+S7gznnmeS3wfuAMYDfwqwBJvg74PeCvGPS0NgNXJ/mettzjwI8x6C1d2Kb/1zbtt4AfSJK2rjXAK9t23gdcmuSMNu1U4AeA3wSoqpdX1c0LeI1aJANmlaiqh4GXAQX8X2A2ye4k64dmu6D1Co4+PtnavxF4sKoeG2VbSV4PTAJvm2fWP62q36+qxxl8yM9r7S8BJqrq51qP6FOt5ivaa9lXVR+rqseq6iDwf4D/0Jb9k/Ya/30b/34Gu34PVNUh4I+By9u0S9vr2jfK69LSM2BWkaraX1U/XFXnAOcCZzE4rnLUx6rqjKHHv2rtDwFnjnLQNslWYAdwWVU92Nr+09BB4z8Ymv2zQ8//EXhG28bzgLOGww74aQbHfkjyrUluSvLZJA8zOC50ZnuNxaC38rq23h9ksLt21DRwZXt+Ja33ouVhwKxSVXUX8F4GQTOfW4AvAlufbKYklzLoaXxvVf310LauHTpofNkI27sPuOeYsHt2Vb2qTX8ncBewqapOZxA+GVr+OuD7kzyPwQHn4eMuvwv82yTnAq/hq8NHJ5kBs0okeWGSNyc5p40/l8F/+Y/Nt2xVfR7478CvJdma5JlJntaO5fxCW9/FDD6s31dVty2y3NuAh5P8VJJvSHJKknOPnlIHng08DHwhyQuB/3JMvR8HZoF3AR+pqs8NTfsicAODYzW3VdW9i6xVi2DArB6PMPhvfmuSRxkEyx3Am4fmuXCO62BeAlBVbwfeBPwMgw/vfcCPMOgRAPws8Bzg94+zOzSydkzme4HzgXuABxmExXPaLD/OYNfnEQY9pt+eYzXXAa9gECTHmgb+De4eLbv4g1PjI8kXGVzX8stV9bPLXc9KleSbGexifVM7+D3XPK8HfpHB2bMXtQPNWmIGjFaVdgr87cDpVfWfl7uep7qxutRbejJJTgMOA59mcIpay8wejKRuPMgrqZuTuot05pln1saNG0/mJiWdBPv27XuwqiaObT+pAbNx40ZmZmZO5iYlnQRJPj1Xu7tIkroxYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkrqZN2CSvCDJ7UOPh5NcnWRtkj1JDrThQn7FXtJTwLxfFaiquxn88hhJTgE+w+D2FtuBvVW1I8n2Nv5TS1XYxu0fWqpVfY2DO17dbd2SvmKhu0ibgU9W1acZ3PFvurVPM88PRkt66llowFzB4LdQAda3+9DQhuuWsjBJ42/kgGm37nwt8DsL2UCSbe2G6jOzs7MLrU/SGFtID+Yy4C+r6nAbP5xkA0AbHplroaraWVWTVTU5MfE1PxchaRVbSMC8jq/sHsHgXsNT7fkUsGupipK0OowUMEmeCVwC3DjUvAO4JMmBNm3H0pcnaZyN9It2VfWPDG6QPtz2EIOzSpI0J6/kldSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1M+qN185IckOSu5LsT3JhkrVJ9iQ50IZrehcrabyM2oP5JeDDVfVC4DxgP7Ad2FtVm4C9bVySnjBvwCQ5Hfgu4N0AVfWlqvocsAWYbrNNA1v7lChpXI3Sg/kWYBb4jSQfT/KuJKcB66vqEEAbrptr4STbkswkmZmdnV2ywiWtfKMEzKnAdwDvrKoXA4+ygN2hqtpZVZNVNTkxMXGCZUoaR6MEzP3A/VV1axu/gUHgHE6yAaANj/QpUdK4mjdgquqzwH1JXtCaNgN/A+wGplrbFLCrS4WSxtapI87334Brkzwd+BTwegbhdH2Sq4B7gcv7lChpXI0UMFV1OzA5x6TNS1qNpFXFK3kldWPASOrGgJHUjQEjqRsDRlI3BoykbgwYSd0YMJK6MWAkdWPASOrGgJHUjQEjqRsDRlI3BoykbgwYSd0YMJK6MWAkdWPASOrGgJHUjQEjqRsDRlI3Boykbka6bUmSg8AjwOPAY1U1mWQt8NvARuAg8B+r6h/6lClpHC2kB/PdVXV+VR29P9J2YG9VbQL2soD7VUt6aljMLtIWYLo9nwa2LroaSavKqAFTwEeT7EuyrbWtr6pDAG24bq4Fk2xLMpNkZnZ2dvEVSxobo96b+qKqeiDJOmBPkrtG3UBV7QR2AkxOTtYJ1ChpTI3Ug6mqB9rwCPBB4KXA4SQbANrwSK8iJY2neQMmyWlJnn30OfBK4A5gNzDVZpsCdvUqUtJ4GmUXaT3wwSRH5/+tqvpwkr8Ark9yFXAvcHm/MiWNo3kDpqo+BZw3R/tDwOYeRUlaHbySV1I3BoykbgwYSd0YMJK6MWAkdWPASOrGgJHUjQEjqRsDRlI3BoykbgwYSd0YMJK6MWAkdWPASOrGgJHUjQEjqRsDRlI3BoykbgwYSd0YMJK6MWAkdWPASOpm5IBJckqSjye5qY2vTbInyYE2XNOvTEnjaCE9mDcC+4fGtwN7q2oTsLeNS9ITRgqYJOcArwbeNdS8BZhuz6eBrUtamaSxN2oP5h3ATwL/MtS2vqoOAbThuqUtTdK4mzdgkrwGOFJV+05kA0m2JZlJMjM7O3siq5A0pkbpwVwEvDbJQeD9wMVJ3gccTrIBoA2PzLVwVe2sqsmqmpyYmFiisiWNg3kDpqquqapzqmojcAXwh1V1JbAbmGqzTQG7ulUpaSwt5jqYHcAlSQ4Al7RxSXrCqQuZuapuBm5uzx8CNi99SZJWC6/kldSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkroxYCR1M8q9qZ+R5LYkf5XkziRvbe1rk+xJcqAN1/QvV9I4GaUH88/AxVV1HnA+cGmSC4DtwN6q2gTsbeOS9IRR7k1dVfWFNvq09ihgCzDd2qeBrT0KlDS+RjoGk+SUJLcDR4A9VXUrsL6qDgG04brjLLstyUySmdnZ2SUqW9I4GClgqurxqjofOAd4aZJzR91AVe2sqsmqmpyYmDjBMiWNowWdRaqqzwE3A5cCh5NsAGjDI0tdnKTxNspZpIkkZ7Tn3wC8ArgL2A1MtdmmgF2dapQ0pk4dYZ4NwHSSUxgE0vVVdVOSW4Drk1wF3Atc3rFOSWNo3oCpqk8AL56j/SFgc4+iJK0OXskrqRsDRlI3BoykbkY5yLvqbNz+oS7rPbjj1V3WK40rezCSujFgJHVjwEjqxoCR1I0BI6kbA0ZSNwaMpG4MGEndGDCSujFgJHVjwEjqxoCR1I0BI6kbA0ZSNwaMpG4MGEndGDCSujFgJHUzyo3Xnpvkj5LsT3Jnkje29rVJ9iQ50IZr+pcraZyM0oN5DHhzVX0bcAHwhiQvArYDe6tqE7C3jUvSE+YNmKo6VFV/2Z4/AuwHzga2ANNttmlga6caJY2pBR2DSbKRwV0ebwXWV9UhGIQQsO44y2xLMpNkZnZ2dpHlShonIwdMkmcBHwCurqqHR12uqnZW1WRVTU5MTJxIjZLG1EgBk+RpDMLl2qq6sTUfTrKhTd8AHOlToqRxNcpZpADvBvZX1duHJu0GptrzKWDX0pcnaZyNcmfHi4AfAv46ye2t7aeBHcD1Sa4C7gUu71KhpLE1b8BU1Z8COc7kzUtbjqTVxCt5JXVjwEjqxoCR1I0BI6kbA0ZSNwaMpG4MGEndGDCSujFgJHVjwEjqxoCR1I0BI6kbA0ZSNwaMpG4MGEndGDCSujFgJHVjwEjqxoCR1M0oP/qtZbZx+4e6rfvgjld3W7dkD0ZSNwaMpG5GufHae5IcSXLHUNvaJHuSHGjDNX3LlDSORunBvBe49Ji27cDeqtoE7G3jkvRV5g2Yqvpj4O+Pad4CTLfn08DWpS1L0mpwosdg1lfVIYA2XHe8GZNsSzKTZGZ2dvYENydpHHU/yFtVO6tqsqomJyYmem9O0gpyogFzOMkGgDY8snQlSVotTjRgdgNT7fkUsGtpypG0moxymvo64BbgBUnuT3IVsAO4JMkB4JI2LklfZd6vClTV644zafMS1yJplfFKXkndGDCSujFgJHVjwEjqxoCR1I0BI6kbA0ZSN/5k5hLq+dOW0jiyByOpGwNGUjfuIj3F9dqt824FAnswkjoyYCR1Y8BI6saAkdSNASOpGwNGUjcGjKRuDBhJ3RgwkrrxSl6NnXG8+njcvgi7VO+FPRhJ3RgwkrpZVMAkuTTJ3Un+Lsn2pSpK0upwwgGT5BTg14DLgBcBr0vyoqUqTNL4W0wP5qXA31XVp6rqS8D7gS1LU5ak1WAxZ5HOBu4bGr8f+M5jZ0qyDdjWRr+Q5O6hyWcCDy6ihuUyrnXDSao9P7/kq+xed4eaYUz/VvLzC677eXM1LiZgMkdbfU1D1U5g55wrSGaqanIRNSyLca0bxrd26z65lqruxewi3Q88d2j8HOCBxZUjaTVZTMD8BbApyfOTPB24Ati9NGVJWg1OeBepqh5L8iPAR4BTgPdU1Z0LXM2cu05jYFzrhvGt3bpPriWpO1Vfc9hEkpaEV/JK6saAkdTNsgXMuHzNIMlzk/xRkv1J7kzyxta+NsmeJAfacM1y1zqXJKck+XiSm9r4iq87yRlJbkhyV3vfLxyTun+s/Y3ckeS6JM9YqXUneU+SI0nuGGo7bq1Jrmmf1buTfM+o21mWgBmzrxk8Bry5qr4NuAB4Q6t1O7C3qjYBe9v4SvRGYP/Q+DjU/UvAh6vqhcB5DOpf0XUnORv4UWCyqs5lcOLjClZu3e8FLj2mbc5a29/7FcC3t2V+vX2G51dVJ/0BXAh8ZGj8GuCa5ajlBGrfBVwC3A1saG0bgLuXu7Y5aj2n/aFcDNzU2lZ03cDpwD20ExBD7Su97qNXtq9lcHb2JuCVK7luYCNwx3zv8bGfTwZnji8cZRvLtYs019cMzl6mWkaWZCPwYuBWYH1VHQJow3XLWNrxvAP4SeBfhtpWet3fAswCv9F27d6V5DRWeN1V9RngbcC9wCHg81X1UVZ43cc4Xq0n/HldroAZ6WsGK0mSZwEfAK6uqoeXu575JHkNcKSq9i13LQt0KvAdwDur6sXAo6yc3YrjascrtgDPB84CTkty5fJWtWRO+PO6XAEzVl8zSPI0BuFybVXd2JoPJ9nQpm8AjixXfcdxEfDaJAcZfNP94iTvY+XXfT9wf1Xd2sZvYBA4K73uVwD3VNVsVX0ZuBH4d6z8uocdr9YT/rwuV8CMzdcMkgR4N7C/qt4+NGk3MNWeTzE4NrNiVNU1VXVOVW1k8P7+YVVdycqv+7PAfUle0Jo2A3/DCq+bwa7RBUme2f5mNjM4OL3S6x52vFp3A1ck+fokzwc2AbeNtMZlPMD0KuBvgU8Cb1nuA15PUufLGHQHPwHc3h6vAr6RwQHUA224drlrfZLX8HK+cpB3xdcNnA/MtPf8d4E1Y1L3W4G7gDuA3wS+fqXWDVzH4FjRlxn0UK56slqBt7TP6t3AZaNux68KSOrGK3kldWPASOrGgJHUjQEjqRsDRlI3BoykbgwYSd38f8B7xl0DZxhAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 1\n",
    "clusters = deepcopy(n_nodes)\n",
    "overlap_score = deepcopy(n_nodes)\n",
    "best_matched_cluster_info = deepcopy(n_nodes)\n",
    "best_matched_complex_info = deepcopy(n_nodes)\n",
    "cluster_cluster_score = deepcopy(n_nodes)\n",
    "\n",
    "for l in range(len(exp_cond)):\n",
    "    mcl = MCL_Cluster(merge_threshold=0.25, min_complex_size=3, max_complex_size=100, split_depth=3)\n",
    "    cur_cluster = mcl.run(edges[l], e_w=w[l], dim=n_nodes[l], diag=1.0)\n",
    "    cur_overlap_score = mcl.get_ove_score(cur_cluster, complexes[l])\n",
    "\n",
    "    cur_best_matched_cluster = cur_overlap_score.argmax(0) # best matched cluster for each complex\n",
    "    cur_best_matched_complex = cur_overlap_score.argmax(1) # best matched complex for each cluster\n",
    "    cur_best_matched_cluster_score = cur_overlap_score.max(0) # overlap perc between best matched cluster and each complex\n",
    "    cur_best_matched_complex_score = cur_overlap_score.max(1) # overlap perc between best matched complex and each cluster\n",
    "\n",
    "    cur_best_matched_cluster_info = [tuple([i, cur_best_matched_cluster[i], \n",
    "                                            cur_best_matched_cluster_score[i]]) for i in range(len(cur_best_matched_cluster))] # 0:complex id, 1:best cluster id, 2:overlap perc\n",
    "    cur_best_matched_complex_info = [tuple([i, cur_best_matched_complex[i], \n",
    "                                            cur_best_matched_complex_score[i]]) for i in range(len(cur_best_matched_complex))] # 0:cluster id, 1:best complex id, 2:overlap perc\n",
    "    cur_best_matched_cluster_info = sorted(cur_best_matched_cluster_info, key=lambda x: x[2], reverse=True)\n",
    "    cur_best_matched_complex_info = sorted(cur_best_matched_complex_info, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    cur_cluster_cluster_score = mcl.get_ove_score(cur_cluster, cur_cluster)\n",
    "\n",
    "    clusters[l] = cur_cluster\n",
    "    overlap_score[l] = cur_overlap_score\n",
    "    best_matched_cluster_info[l] = cur_best_matched_cluster_info\n",
    "    best_matched_complex_info[l] = cur_best_matched_complex_info\n",
    "    cluster_cluster_score[l] = cur_cluster_cluster_score\n",
    "\n",
    "    print(exp_name + ' ' + exp_cond[l])\n",
    "    print(len(cur_cluster), min([len(i) for i in cur_cluster]), max([len(i) for i in cur_cluster]))\n",
    "\n",
    "f, a = plt.subplots(1, 1, figsize=(4,4))\n",
    "a.hist([len(i) for i in clusters[0]])\n",
    "a.set_title(datas[0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c53f4-25f9-4155-9739-05267e5dd34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "054c7ef4-94b2-4929-bb93-863ee26c0fc4",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2b1c636-0302-465c-9432-64dcf356b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = './analysis'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)\n",
    "\n",
    "out_path = './analysis/Cluster'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "696dfbc0-b31a-49da-b4e4-df071d808b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save clusters\n",
    "for l in range(len(exp_cond)):\n",
    "    with open('./analysis/Cluster/' + '_'.join([exp_cond[l], 'clusters.txt']), 'w') as output:\n",
    "        for line in clusters[l]:\n",
    "            s = \" \".join(map(str, line))\n",
    "            output.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7aeaef-5fba-43dc-90b2-7e847e05fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save cluster info\n",
    "for l in range(len(exp_cond)):\n",
    "    np.save('./analysis/Cluster/' + '_'.join([exp_cond[l], 'overlap_score.npy']), overlap_score)\n",
    "    np.save('./analysis/Cluster/' + '_'.join([exp_cond[l], 'best_matched_cluster_info.npy']), best_matched_cluster_info)\n",
    "    np.save('./analysis/Cluster/' + '_'.join([exp_cond[l], 'best_matched_complex_info.npy']), best_matched_complex_info)\n",
    "    np.save('./analysis/Cluster/' + '_'.join([exp_cond[l], 'cluster_cluster_score.npy']), cluster_cluster_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821581a0-8d0f-4500-8bc1-01ad0b51c170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5326ae79-e299-4a08-af48-38df594ff5fa",
   "metadata": {},
   "source": [
    "## Load ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7210a02-6ed7-4db8-b3dc-753dd81a535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n",
      "115 231 357 482 509 611 692 693 700 728\n",
      "880 896 897\n",
      "30 63 95 168 170 181 201 219 248 419 431 464 476 514 515 518 531 536 546 600 610 612 636 697 705 714 724 790 884 933 975 1007 1115 1215 1217 1368 1391 1425 1480 1485 1493 1495 1499 1550 1616 1640 1659 1660 1697 1715 1722 1744 1752 1763 1767 1778 1823 1843 1855 1858 1859 1867 1870 1887 1897 1984 2058 2144 2181 2228 2255 2288 2311 2315 2319 2320 2323 2340 2351 2390 2392 2393 2409 2419 2429 2469 2565 2710 2830 2842 3071 3189 3229 3342 3624 3740 3751 3753 3754 3952\n",
      "356 434 470 499 601\n",
      "953 1544 1757 1879 1896 1910 1912 1913 1923 2077 2082 2112 2204 2260 2458 2460 2567 2568 2653 2657 2696 2831 2960 2994 3191 3243 3278 3280 3317 3318 3319 3325 3348 3433 3461 3471 3514 3586 3633 3634 3728 3762 3785 3873 3893 3951 3962\n",
      "**************************************************\n",
      "127\n",
      "(115, 231, 357, 482, 509, 611, 692, 693, 700, 728)\n",
      "(880, 896, 897)\n",
      "(30, 63, 95, 168, 170, 181, 201, 219, 248, 419, 431, 464, 476, 514, 515, 518, 531, 536, 546, 600, 610, 612, 636, 697, 705, 714, 724, 790, 884, 933, 975, 1007, 1115, 1215, 1217, 1368, 1391, 1425, 1480, 1485, 1493, 1495, 1499, 1550, 1616, 1640, 1659, 1660, 1697, 1715, 1722, 1744, 1752, 1763, 1767, 1778, 1823, 1843, 1855, 1858, 1859, 1867, 1870, 1887, 1897, 1984, 2058, 2144, 2181, 2228, 2255, 2288, 2311, 2315, 2319, 2320, 2323, 2340, 2351, 2390, 2392, 2393, 2409, 2419, 2429, 2469, 2565, 2710, 2830, 2842, 3071, 3189, 3229, 3342, 3624, 3740, 3751, 3753, 3754, 3952)\n",
      "(356, 434, 470, 499, 601)\n",
      "(953, 1544, 1757, 1879, 1896, 1910, 1912, 1913, 1923, 2077, 2082, 2112, 2204, 2260, 2458, 2460, 2567, 2568, 2653, 2657, 2696, 2831, 2960, 2994, 3191, 3243, 3278, 3280, 3317, 3318, 3319, 3325, 3348, 3433, 3461, 3471, 3514, 3586, 3633, 3634, 3728, 3762, 3785, 3873, 3893, 3951, 3962)\n"
     ]
    }
   ],
   "source": [
    "with open('./analysis/Cluster/' + '_'.join([exp_cond[0], 'clusters.txt']),'r') as fh:\n",
    "    a = []\n",
    "    for oneline in fh:\n",
    "        oneline = oneline.strip()\n",
    "        a.append(oneline)\n",
    "print(len(a))\n",
    "print(*a[:5], sep='\\n')\n",
    "print('*'*50)\n",
    "print(len(clusters[0]))\n",
    "print(*clusters[0][:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834acc09-4637-4082-ba0f-d3ac4f023255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aeb16-d93e-4eb6-833c-5af782ea7693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac8ab8-805c-45cd-89af-e24c8a308e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5d096-1032-435b-8d8a-335e997d7145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e859f0f-1003-432f-a15d-778d8a365390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
