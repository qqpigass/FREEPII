{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec05b836-2cc1-4fb3-a26f-1e1ef1d45119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pyreadr\n",
    "import random\n",
    "\n",
    "from itertools import combinations, chain\n",
    "from collections import OrderedDict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import show, cm, axis\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from torchmetrics import Accuracy\n",
    "from copy import deepcopy\n",
    "accuracy = Accuracy(task=\"binary\")\n",
    "\n",
    "from dynamicTreeCut import cutreeHybrid\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "np.set_printoptions(edgeitems=10, linewidth=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1135d6-49c2-464d-a2a9-a7ef39ac2d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a9d3ab-247d-4feb-bd01-837541962f35",
   "metadata": {},
   "source": [
    "# Train model and generate predicted PPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cb3e4-b2a1-4ff3-9063-726bc4d5b515",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3999175d-2f5c-4954-b9e0-f03b906868be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Setting\n",
    "DEVICE = 'cpu' # or 'cuda:0'\n",
    "\n",
    "exp_name = 'PXD002892'\n",
    "exp_cond = 'SEC2-heavy'\n",
    "emb_size = [200, 256]\n",
    "\n",
    "load_dir1 = '/'.join(['/FREEPII_github/input', exp_name])\n",
    "load_dir2 = '/'.join(['/FREEPII_github/EPF-data', exp_name, exp_cond])\n",
    "\n",
    "dict_path = load_dir1 + '/name_idx_dict_' + exp_cond + '.pickle'\n",
    "comp_path = '/FREEPII_github/Protein complex-data/Complexes_gene_Human_filter.rds'\n",
    "print(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ae6f703-686f-4a93-8eaa-17570c36b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read data\n",
    "train_edge    = torch.tensor( np.load(load_dir1 + '/ref_edge_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "heldout_edge  = torch.tensor( np.load(load_dir1 + '/heldout_edge_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "delfold_edge  = torch.tensor( np.load(load_dir1 + '/delfold_edge_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "exp_edge      = torch.tensor( np.load(load_dir1 + '/exp_edge_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "train_label   = torch.tensor( np.load(load_dir1 + '/ref_label_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "heldout_label = torch.tensor( np.load(load_dir1 + '/heldout_label_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "delfold_label = torch.tensor( np.load(load_dir1 + '/delfold_label_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "train_mask    = torch.tensor( np.load(load_dir1 + '/train_mask_' + exp_cond + '.npz')['arr_0'] ).to(torch.bool).to(DEVICE)\n",
    "test_mask     = torch.tensor( np.load(load_dir1 + '/test_mask_' + exp_cond + '.npz')['arr_0'] ).to(torch.bool).to(DEVICE)\n",
    "                             \n",
    "print(train_edge.shape, heldout_edge.shape, delfold_edge.shape, exp_edge.shape)\n",
    "print(train_label.shape, heldout_label.shape, delfold_label.shape)\n",
    "print(train_mask.shape, test_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81ce1b4a-97c6-4e48-a336-518c4b166c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "epf = list(pyreadr.read_r(load_dir2 + '/' + exp_cond + '.rds').values())[0].values\n",
    "pad = np.zeros((epf.shape[0], emb_size[0] - epf.shape[1]))\n",
    "print(epf.shape, pad.shape)\n",
    "\n",
    "epf = torch.tensor( np.concatenate([epf, pad], 1).reshape(-1, emb_size[0], 1)).to(torch.float32).to(DEVICE)\n",
    "print(epf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a06e159-d6b2-4908-8946-4b5d913e0c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_in_exp_idx     = torch.tensor( np.load(load_dir1 + '/seq_in_exp_idx_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "seq_off_exp_idx    = torch.tensor( np.load(load_dir1 + '/seq_off_exp_idx_' + exp_cond + '.npz')['arr_0'] ).to(torch.long).to(DEVICE)\n",
    "seq                = torch.tensor( np.load(load_dir1 + '/feature_seq_FCGR_' + str(int(emb_size[1]**0.5)) + 'x_' + exp_cond + '.npz')['arr_0'] ).to(torch.float32)\n",
    "seq                = torch.unsqueeze(seq, -1).to(DEVICE)\n",
    "\n",
    "print(seq_in_exp_idx.shape, seq_off_exp_idx.shape)\n",
    "print(seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d92509e3-1bf0-49d8-9fe4-e19b48ee95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_mask.sum(1))\n",
    "print('*'*50)\n",
    "\n",
    "for idx in range(train_mask.shape[0]):\n",
    "    #### Create val mask\n",
    "    val_idx = random.sample((train_mask[idx]==1).nonzero().reshape(-1).tolist(), round(int((train_mask[idx]==1).sum())*0.2))\n",
    "    train_idx = list(set((train_mask[idx]==1).nonzero().reshape(-1).tolist())^set(val_idx))\n",
    "    print(val_idx[:5], len(val_idx))\n",
    "    print(train_idx[:5], len(train_idx))\n",
    "    \n",
    "    cur_val_mask = torch.zeros(train_mask[idx].shape[0])\n",
    "    cur_val_mask[val_idx] = 1\n",
    "    cur_val_mask = cur_val_mask.bool()\n",
    "    print(cur_val_mask, cur_val_mask.sum())\n",
    "\n",
    "    cur_train_mask = torch.zeros(train_mask[idx].shape[0])\n",
    "    cur_train_mask[train_idx] = 1\n",
    "    cur_train_mask = cur_train_mask.bool()\n",
    "    print(cur_train_mask, cur_train_mask.sum())\n",
    "    \n",
    "    if idx==0:\n",
    "        val_mask = cur_val_mask.reshape(1,-1)\n",
    "    else:\n",
    "        val_mask = torch.cat((val_mask, cur_val_mask.reshape(1,-1)), 0)\n",
    "    \n",
    "    train_mask[idx] = cur_train_mask\n",
    "    print('*'*50)\n",
    "\n",
    "print(val_mask.sum(1))\n",
    "print(train_mask.sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1637ff-f414-4b98-8fc4-68742d3687ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd641121-c319-436f-989d-878dac2aabbc",
   "metadata": {},
   "source": [
    "## Model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0036ce4b-15b7-4914-85b9-5ef616a21fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding_layer(nn.Module):\n",
    "    def __init__(self, dim_1, dim_2):\n",
    "        super(embedding_layer, self).__init__()\n",
    "        self.Emb = nn.Embedding(dim_1, dim_2, max_norm=True)\n",
    "    \n",
    "    def forward(self, in_idx, off_idx, in_emb):\n",
    "        in_emb = in_emb.squeeze(-1)\n",
    "        all_tokens = len(list(set(in_idx.tolist() + off_idx.tolist())))\n",
    "        out_emb = self.Emb(torch.arange(all_tokens))\n",
    "        \n",
    "        past_off_idx = []\n",
    "        for idx in range(all_tokens):\n",
    "            if idx in in_idx:\n",
    "                out_emb[idx, :] += in_emb[(idx - len(past_off_idx)), ]\n",
    "            else:\n",
    "                past_off_idx.append(idx)\n",
    "        return out_emb.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4a7b9d-02a3-4ce6-b285-91c3f68c5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hid_dim=8, dropout=0.1, head_num=8, head_dim=32, max_size=emb_size*2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        self.conv = nn.Conv1d(1, hid_dim, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(max_size*(hid_dim + 1), 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, inp, e_idx, bias=None):\n",
    "        x = self.conv(inp.permute(0, 2, 1)).permute(0, 2, 1) + inp.repeat(1, 1, self.hid_dim)\n",
    "        \n",
    "        x_e1 = inp[e_idx[:, 0]] - inp[e_idx[:, 1]]\n",
    "        x_e2 = x[e_idx[:, 0]] - x[e_idx[:, 1]]\n",
    "        x_e = torch.cat((x_e1 , x_e2), -1).flatten(start_dim=1)\n",
    "        \n",
    "        x_e = F.dropout(F.relu(self.fc1(x_e)), p=self.dropout)\n",
    "        x_e = F.dropout(F.relu(self.fc2(x_e)), p=self.dropout)\n",
    "        w = torch.sigmoid(self.fc3(x_e)).squeeze()\n",
    "        feat = x.flatten(start_dim=1)\n",
    "        \n",
    "        if bias != None:\n",
    "            feat += bias\n",
    "        \n",
    "        return feat, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb1dae6-983c-481b-a395-822eaed81be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderr(nn.Module):\n",
    "    def __init__(self, hid_dim=16, max_size=emb_size, dropout=0.1, all_tokens=20):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.emb = embedding_layer(all_tokens, emb_size[1])\n",
    "        self.enc = Encoder(hid_dim=hid_dim, max_size=sum(max_size), dropout=dropout)\n",
    "        \n",
    "    def forward(self, inp_e, inp_s, e_idx, in_list=None, off_list=None):\n",
    "        \n",
    "        if off_list is not None:\n",
    "            inp_s = self.emb(in_list, off_list, inp_s)\n",
    "        \n",
    "        inp = torch.cat([inp_e, inp_s], 1)\n",
    "        x, weight = self.enc(inp, e_idx, bias=None)\n",
    "        \n",
    "        return x, weight, inp[:, inp_e.shape[1]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a3b3b-0884-43bf-90e6-a5224c906f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcfd4882-2a38-46f2-8667-dacfc9e61bd9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3babe8-b36f-4e31-96ea-c45d50128845",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_weight    = 2.5\n",
    "L2_weight         = 0.25\n",
    "weight_decay      = 2.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a2b7fec-51d4-42fa-8c6b-6a1f6e239319",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = './performance_record'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)\n",
    "\n",
    "out_path = './best_model_weight'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)\n",
    "\n",
    "out_path = './output'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae4bc996-fa19-471a-a033-088e6e3a6186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_loss_list = []\n",
    "val_loss_list   = []\n",
    "train_acc_list  = []\n",
    "val_acc_list    = []\n",
    "\n",
    "for cv_idx in range(5):\n",
    "    model = Encoderr(hid_dim=16, max_size=emb_size, dropout=0.3, all_tokens=max(seq_in_exp_idx.tolist() + seq_off_exp_idx.tolist()) + 1).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=weight_decay)\n",
    "    \n",
    "    print('cur cv: ', cv_idx)\n",
    "    train_loss_list.append([])\n",
    "    val_loss_list.append([])\n",
    "    train_acc_list.append([])\n",
    "    val_acc_list.append([])\n",
    "    best_val_loss_ = float('inf')\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        #### Train =================================================================================================\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        _, train_label_predict, _ = model(epf, seq, train_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "        \n",
    "        cur_train_loss = F.binary_cross_entropy(train_label_predict[train_mask[cv_idx]], train_label[train_mask[cv_idx]].to(torch.float32))\n",
    "        cur_L2_loss = sum([ (v**2).sum()/2 for v in model.parameters() ])\n",
    "        \n",
    "        loss = predict_weight*cur_train_loss + weight_decay*L2_weight*cur_L2_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_train_acc = accuracy(train_label_predict[train_mask[cv_idx]] > 0.5, train_label[train_mask[cv_idx]] > 0.5)\n",
    "        \n",
    "        #### Validataion ===========================================================================================\n",
    "        model.eval()\n",
    "        _, train_label_predict, _ = model(epf, seq, train_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "        \n",
    "        cur_val_loss = F.binary_cross_entropy(train_label_predict[val_mask[cv_idx]], train_label[val_mask[cv_idx]].to(torch.float32))\n",
    "        cur_val_acc = accuracy(train_label_predict[val_mask[cv_idx]] > 0.5, train_label[val_mask[cv_idx]] > 0.5)\n",
    "        \n",
    "        if epoch in [0,49,99]:\n",
    "            print(\"Epoch:\", '%04d' % (epoch + 1), \"all_loss=\", \"{:.4f}\".format(loss))\n",
    "            print(\"train_loss=\", \"{:.4f}\".format(predict_weight*cur_train_loss), \n",
    "                  \"val_loss=\", \"{:.4f}\".format(predict_weight*cur_val_loss),\n",
    "                  \"lossL2=\", \"{:.4f}\".format(weight_decay*L2_weight*cur_L2_loss))\n",
    "        \n",
    "        if cur_val_loss < best_val_loss_:\n",
    "            torch.save(model.state_dict(), './best_model_weight/' + exp_cond + '_cv' + str(cv_idx+1) + '_stat_dict')\n",
    "            best_val_loss_ = cur_val_loss\n",
    "        \n",
    "        if cur_val_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), './best_model_weight/' + exp_cond + '_stat_dict')\n",
    "            best_val_loss = cur_val_loss\n",
    "        \n",
    "        train_loss_list[cv_idx].append(float(cur_train_loss))\n",
    "        val_loss_list[cv_idx].append(float(cur_val_loss))\n",
    "        train_acc_list[cv_idx].append(cur_train_acc)\n",
    "        val_acc_list[cv_idx].append(cur_val_acc)\n",
    "    \n",
    "    #### Evaluation\n",
    "    model_dict = torch.load('./best_model_weight/' + exp_cond + '_cv' + str(cv_idx+1) + '_stat_dict')\n",
    "    model_load = Encoderr(hid_dim=16, max_size=emb_size, dropout=0.3, all_tokens=max(seq_in_exp_idx.tolist() + seq_off_exp_idx.tolist()) + 1).to(DEVICE)\n",
    "    model_load.load_state_dict(model_dict)\n",
    "    model_load.eval()\n",
    "    \n",
    "    print('Evaluation: ')\n",
    "    _, cur_train_pred, _ = model_load(epf, seq, train_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "    _, cur_test_pred, _  = model_load(epf, seq, heldout_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "    _, cur_del_pred, _   = model_load(epf, seq, delfold_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "    \n",
    "    train_tp = (((cur_train_pred > 0.5).to(torch.float32) + (train_label > 0.5).to(torch.float32))==2).sum() / (train_label > 0.5).sum()\n",
    "    train_tn = (((cur_train_pred <= 0.5).to(torch.float32) + (train_label <= 0.5).to(torch.float32))==2).sum() / (train_label <= 0.5).sum()\n",
    "    print('train TP: ', \"{:.4f}\".format(float(train_tp)), ' train TN: ', \"{:.4f}\".format(float(train_tn)))\n",
    "    \n",
    "    test_tp = (((cur_test_pred > 0.5).to(torch.float32) + (heldout_label > 0.5).to(torch.float32))==2).sum() / (heldout_label > 0.5).sum()\n",
    "    test_tn = (((cur_test_pred <= 0.5).to(torch.float32) + (heldout_label <= 0.5).to(torch.float32))==2).sum() / (heldout_label <= 0.5).sum()\n",
    "    print('heldout TP: ', \"{:.4f}\".format(float(test_tp)), ' heldout TN: ', \"{:.4f}\".format(float(test_tn)))\n",
    "    \n",
    "    del_fp = (cur_del_pred > 0.5).sum() / delfold_label.shape[0]\n",
    "    del_tn = (((cur_del_pred <= 0.5).to(torch.float32) + (delfold_label <= 0.5).to(torch.float32))==2).sum() / delfold_label.shape[0]\n",
    "    print('del-fold FP: ', \"{:.4f}\".format(float(del_fp)), ' del-fold TN: ', \"{:.4f}\".format(float(del_tn)))\n",
    "    \n",
    "    if cv_idx==0:\n",
    "        train_pred = cur_train_pred.reshape(1, -1)\n",
    "        test_pred  = cur_test_pred.reshape(1, -1)\n",
    "        del_pred   = cur_del_pred.reshape(1, -1)\n",
    "    else:\n",
    "        train_pred = torch.cat((train_pred, cur_train_pred.reshape(1, -1)), 0)\n",
    "        test_pred = torch.cat((test_pred, cur_test_pred.reshape(1, -1)), 0)\n",
    "        del_pred = torch.cat((del_pred, cur_del_pred.reshape(1, -1)), 0)\n",
    "    \n",
    "    del cur_train_pred\n",
    "    del cur_test_pred\n",
    "    del cur_del_pred\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82df6118-913f-43b1-9e90-f21edd99fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_pred.shape, test_pred.shape, del_pred.shape)\n",
    "\n",
    "#### Mean predictions over cv folds\n",
    "train_pred = train_pred.mean(0)\n",
    "test_pred = test_pred.mean(0)\n",
    "del_pred = del_pred.mean(0)\n",
    "print(train_pred.shape, test_pred.shape, del_pred.shape)\n",
    "\n",
    "np.savez('./output/' + '_'.join([exp_cond, 'train', 'out']), train_pred.detach().numpy())\n",
    "np.savez('./output/' + '_'.join([exp_cond, 'heldout', 'out']), test_pred.detach().numpy())\n",
    "np.savez('./output/' + '_'.join([exp_cond, 'delfold', 'out']), del_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3a4c99-f45e-4e34-9021-384de2caafc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ca03d24-fc9f-4754-8003-fe381ffca598",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c4781fe-475c-44d0-bf67-a93b123cc98f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dict = torch.load('./best_model_weight/' + exp_cond + '_stat_dict')\n",
    "model_load = Encoderr(hid_dim=16, max_size=emb_size, dropout=0.3, all_tokens=max(seq_in_exp_idx.tolist() + seq_off_exp_idx.tolist()) + 1).to(DEVICE)\n",
    "model_load.load_state_dict(model_dict)\n",
    "model_load.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b490660-23f8-4b18-bfa9-5465375e8723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Emb of all edge\n",
    "all_edge = torch.cat([train_edge, heldout_edge, delfold_edge, exp_edge], 0)\n",
    "print(all_edge.shape)\n",
    "\n",
    "emb, all_pred, _ = model_load(epf, seq, all_edge, in_list=seq_in_exp_idx, off_list=seq_off_exp_idx)\n",
    "print(all_pred.shape, emb.shape)\n",
    "\n",
    "np.savez('./output/' + '_'.join([exp_cond, 'all', 'out']), all_pred.detach().numpy())\n",
    "np.savez('./output/' + '_'.join([exp_cond, 'all_emb', 'out']), emb.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dc695-e261-4f6e-b004-ed83959e9e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6345d7-8525-4167-99a7-22ef07ea70be",
   "metadata": {},
   "source": [
    "## Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f84404-e0be-4b11-9987-ad1e876aaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.is_tensor( train_loss_list[0][0] ):\n",
    "    train_loss_list = [[float(i) for i in train_loss_list[j]] for j in range(len(train_loss_list))]\n",
    "if torch.is_tensor( val_loss_list[0][0] ):\n",
    "    val_loss_list = [[float(i) for i in val_loss_list[j]] for j in range(len(val_loss_list))]\n",
    "\n",
    "print(torch.is_tensor( train_loss_list[0][0] ))\n",
    "print(torch.is_tensor( val_loss_list[0][0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9591684-95b4-4f7c-9ce2-f94b0616ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = np.array(train_acc_list)\n",
    "val_acc_list = np.array(val_acc_list)\n",
    "train_loss_list = np.array(train_loss_list)\n",
    "val_loss_list = np.array(val_loss_list)\n",
    "print(train_acc_list.shape, val_acc_list.shape, train_loss_list.shape, val_loss_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b69b7596-e6ee-4b5f-92de-d8e37b5b444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(13, 10))\n",
    "ax[0,0].plot(np.arange(100), train_acc_list.T)\n",
    "ax[0,0].legend(['cv'+str(i) for i in range(5)])\n",
    "ax[0,0].set_title('Train acc')\n",
    "ax[0,1].plot(np.arange(100), val_acc_list.T)\n",
    "ax[0,1].legend(['cv'+str(i) for i in range(5)])\n",
    "ax[0,1].set_title('Validation acc')\n",
    "ax[1,0].plot(np.arange(100), train_loss_list.T)\n",
    "ax[1,0].legend(['cv'+str(i) for i in range(5)])\n",
    "ax[1,0].set_title('Train loss')\n",
    "ax[1,1].plot(np.arange(100), val_loss_list.T)\n",
    "ax[1,1].legend(['cv'+str(i) for i in range(5)])\n",
    "ax[1,1].set_title('Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d36f2f2-4343-4bc4-ad69-99d64773bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./performance_record/' + '_'.join([exp_cond, 'train_loss']), train_loss_list)\n",
    "np.save('./performance_record/' + '_'.join([exp_cond, 'val_loss']), val_loss_list)\n",
    "np.save('./performance_record/' + '_'.join([exp_cond, 'train_acc']), train_acc_list)\n",
    "np.save('./performance_record/' + '_'.join([exp_cond, 'val_acc']), val_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0629d5-8162-42d0-841f-ca4b1add856a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1adb8540-67b7-4731-a5a5-3d9deee0de2e",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7f58777-a727-4936-b033-5c2c17d46e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCL_Cluster(object):\n",
    "    def __init__(self, merge_threshold=0.25, min_complex_size=3, max_complex_size=100, split_depth=1):\n",
    "        self.merge_threshold = merge_threshold\n",
    "        self.min_complex_size = min_complex_size\n",
    "        self.max_complex_size = max_complex_size\n",
    "        self.split_depth = split_depth\n",
    "        self.break_num = 0\n",
    "        self.merge_num = 0\n",
    "        self.max_break_num = 0\n",
    "        self.max_merge_num = 7\n",
    "    \n",
    "    def create_edge_adj(self, e_idx, e_w=None, dim=None, diag=None):\n",
    "        self.dim = dim\n",
    "        mat = np.zeros([dim, dim])\n",
    "        if e_w is not None:\n",
    "            if e_idx.shape[0]==2:\n",
    "                mat[e_idx[0], e_idx[1]] = e_w\n",
    "            elif e_idx.shape[1]==2:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = e_w\n",
    "            else:\n",
    "                print('wrong edge dimension')\n",
    "                return\n",
    "        else:\n",
    "            if e_idx.shape[0]==3:\n",
    "                mat[e_idx[0], e_idx[1]] = e_idx[2]\n",
    "            elif e_idx.shape[1]==3:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = e_idx[:,2]\n",
    "            elif e_idx.shape[0]==2:\n",
    "                mat[e_idx[0], e_idx[1]] = 1.0\n",
    "                print('Not give edge weight, use 1.0 as weight')\n",
    "            elif e_idx.shape[1]==2:\n",
    "                mat[e_idx[:,0], e_idx[:,1]] = 1.0\n",
    "                print('Not give edge weight, use 1.0 as weight')\n",
    "        mat += mat.transpose()\n",
    "        if diag is not None:\n",
    "            np.fill_diagonal(mat, diag)\n",
    "        return mat\n",
    "    \n",
    "    def create_tom_adj(self, A):\n",
    "        d = A.shape[0]\n",
    "        L = A.dot(A.T)\n",
    "        K = A.sum(axis=1)\n",
    "\n",
    "        A_tom = np.zeros_like(A)\n",
    "        for i in range(d):\n",
    "            for j in range(i+1, d):  \n",
    "                numerator = L[i, j] + A[i, j]\n",
    "                denominator = min(K[i], K[j]) + 1 - A[i, j]\n",
    "                A_tom[i, j] = numerator / denominator\n",
    "\n",
    "        A_tom += A_tom.T\n",
    "        np.fill_diagonal(A_tom, 0)\n",
    "        return np.nan_to_num(A_tom)\n",
    "    \n",
    "    def iterate(self, A, expand_power, inflate_power):\n",
    "        A_ = np.linalg.matrix_power(A, expand_power)  # expand\n",
    "        A_ = np.power(A_, inflate_power)              # inflate\n",
    "        A_ = A_ / A_.sum(0)                           # col-wise normalization\n",
    "        AA = A_.copy()\n",
    "        A_[A_ < 0.001] = 0                            # pruning\n",
    "        num_cols = AA.shape[1]\n",
    "        col_indices = np.arange(num_cols)\n",
    "        row_indices = A.argmax(axis=0).reshape((num_cols,)) # keep max of each col in original matrix\n",
    "        A_[row_indices, col_indices] = AA[row_indices, col_indices]\n",
    "        return A_\n",
    "    \n",
    "    def MCL_process(self, A, expand_power=2, inflate_power=2, iters=2):\n",
    "        A_ = A / A.sum(0)\n",
    "        for j in range(iters):\n",
    "            A_ = self.iterate(A_, expand_power, inflate_power)\n",
    "        np.fill_diagonal(A_, 0)\n",
    "        return A_\n",
    "    \n",
    "    def label_nodes(self, n_idx, lab):\n",
    "        sidx = lab.argsort()                                    # Get sidx (sorted indices) for label\n",
    "        split_idx = np.flatnonzero(np.diff(lab[sidx]) > 0) + 1  # Get where the sorted version of label changes groups\n",
    "        groups = np.split(n_idx[sidx], split_idx)               # Sort input based on the sidx and split label on split_idx\n",
    "        return [tuple(sorted(i)) for i in groups]\n",
    "    \n",
    "    def remove_subset(self, Cs):\n",
    "        return list(filter(lambda f: not any(set(f) < set(g) for g in Cs), Cs))\n",
    "    \n",
    "    def get_modularity(self, A, Cs, gamma=1.0):\n",
    "        Lc_m = [( np.array(list(combinations(Cs[j], 2))).shape[0]  / ((np.array(A.nonzero()).T.shape[0] - A.shape[0])/2) ) for j in range(len(Cs))]\n",
    "        kc_2m = [(( sum([A[k, :].sum() for k in Cs[j]]) / (np.array(A.nonzero()).T.shape[0] - A.shape[0]) )**2)*gamma for j in range(len(Cs))]\n",
    "        M = np.array(Lc_m)-np.array(kc_2m)\n",
    "        return M\n",
    "    \n",
    "    def extract_clusters(self, A, keep_idx=None):\n",
    "        print('extract clusters')\n",
    "        \n",
    "        zero_idx = np.array(list(set(np.where(A.sum(0)!=0)[0])&set(np.where(A.sum(1)!=0)[0])))\n",
    "        \n",
    "        I = self.create_tom_adj(np.power(A, 2.5))\n",
    "        \n",
    "        if keep_idx is None:\n",
    "            keep_idx = zero_idx\n",
    "        else:\n",
    "            keep_idx = np.array(list( set(keep_idx) & set(zero_idx) ))\n",
    "        \n",
    "        A_ = A[keep_idx, :][:, keep_idx]\n",
    "        A_ = self.MCL_process(A_, iters=3)\n",
    "        I_ = I[keep_idx, :][:, keep_idx]\n",
    "        \n",
    "        imp_1 = A_*0.3 + I_*0.7\n",
    "        imp_2 = A_*0.1 + I_*0.9\n",
    "        \n",
    "        dist_1 = pdist(imp_1, metric='cosine')\n",
    "        dist_2 = pdist(imp_2, metric='cosine')\n",
    "        \n",
    "        dist_1[~np.isfinite(dist_1)] = 1.0\n",
    "        dist_2[~np.isfinite(dist_2)] = 1.0\n",
    "        \n",
    "        link_1 = linkage(dist_1, 'ward')\n",
    "        link_2 = linkage(dist_2, 'ward')\n",
    "        \n",
    "        Cs_lab_1 = cutreeHybrid(link_1, dist_1, minClusterSize=(self.min_complex_size), \n",
    "                                deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "        Cs_lab_2 = cutreeHybrid(link_2, dist_2, minClusterSize=(self.min_complex_size), \n",
    "                                deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "        Cs_lab_1 = Cs_lab_1['labels']\n",
    "        Cs_lab_2 = Cs_lab_2['labels']\n",
    "        \n",
    "        Cs_1 = self.label_nodes(keep_idx, Cs_lab_1)\n",
    "        Cs_2 = self.label_nodes(keep_idx, Cs_lab_2)\n",
    "        Cs = Cs_1 + Cs_2\n",
    "        \n",
    "        small_Cs = [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "        large_Cs = [j for j in Cs if len(j) > self.max_complex_size]\n",
    "        \n",
    "        while len(large_Cs) > 0 and self.break_num <= self.max_break_num:\n",
    "            print('break clusters, break iter: ', self.break_num)\n",
    "            \n",
    "            idx = 0\n",
    "            max_idx = len(large_Cs)\n",
    "            max_idx_ori = len(large_Cs)\n",
    "            while (idx < max_idx)&(idx < max_idx_ori):\n",
    "                cur_large_C = large_Cs[idx]\n",
    "                \n",
    "                keep_idx = np.array(list( set(cur_large_C) & set(zero_idx) ))\n",
    "                \n",
    "                A_ = A[keep_idx, :][:, keep_idx]\n",
    "                A_ = self.MCL_process(A_, iters=5)\n",
    "                I_ = I[keep_idx, :][:, keep_idx]\n",
    "                \n",
    "                imp_1 = A_*0.3 + I_*0.7\n",
    "                imp_2 = A_*0.1 + I_*0.9\n",
    "\n",
    "                dist_1 = pdist(imp_1, metric='cosine')\n",
    "                dist_2 = pdist(imp_2, metric='cosine')\n",
    "\n",
    "                dist_1[~np.isfinite(dist_1)] = 1.0\n",
    "                dist_2[~np.isfinite(dist_2)] = 1.0\n",
    "\n",
    "                link_1 = linkage(dist_1, 'ward')\n",
    "                link_2 = linkage(dist_2, 'ward')\n",
    "\n",
    "                Cs_lab_1 = cutreeHybrid(link_1, dist_1, minClusterSize=(self.min_complex_size), \n",
    "                                        deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "                Cs_lab_2 = cutreeHybrid(link_2, dist_2, minClusterSize=(self.min_complex_size), \n",
    "                                        deepSplit=self.split_depth, respectSmallClusters=True, verbose=False, pamStage=True)\n",
    "                Cs_lab_1 = Cs_lab_1['labels']\n",
    "                Cs_lab_2 = Cs_lab_2['labels']\n",
    "\n",
    "                Cs_1 = self.label_nodes(keep_idx, Cs_lab_1)\n",
    "                Cs_2 = self.label_nodes(keep_idx, Cs_lab_2)\n",
    "                Cs = Cs_1 + Cs_2\n",
    "                \n",
    "                small_Cs = small_Cs + [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "                large_Cs = large_Cs + [j for j in Cs if len(j) > self.max_complex_size]\n",
    "                large_Cs = self.remove_subset(large_Cs)\n",
    "                idx += 1\n",
    "                max_idx = len(large_Cs)\n",
    "            self.break_num += 1\n",
    "        self.break_num = 0\n",
    "        # small_Cs = self.remove_subset(small_Cs)\n",
    "        return small_Cs\n",
    "    \n",
    "    def get_ove_score(self, list1, list2):\n",
    "        if type(list1[0])!=list and type(list1[0])!=tuple:\n",
    "            list1 = [list1]\n",
    "        if type(list2[0])!=list and type(list2[0])!=tuple:\n",
    "            list2 = [list2]\n",
    "        \n",
    "        s1 = np.array([[len(set(list1[k])&set(tuple(j)))**2 for j in list2 ] for k in range(len(list1))])\n",
    "        s2 = np.array([[len(list1[k])*len(j) for j in list2 ] for k in range(len(list1))])\n",
    "        ove_score = np.nan_to_num(s1 / s2)\n",
    "        if ove_score.shape[0] > 1:\n",
    "            np.fill_diagonal(ove_score, 0)\n",
    "            return ove_score\n",
    "        else:\n",
    "            return ove_score.reshape(-1)\n",
    "    \n",
    "    def merge_clusters(self, Cs, A):\n",
    "        if_merge = True\n",
    "        while if_merge:\n",
    "            print('merge clusters, merge iter: ', self.merge_num)\n",
    "            ove_score = self.get_ove_score(Cs, Cs)\n",
    "            ove_score = np.triu(ove_score, k=1) # keep only upper traingular values\n",
    "            \n",
    "            ## Combind cluster id with their overlap score\n",
    "            ove_clusters_pair_id_w = list(zip(zip(np.arange(ove_score.shape[0]), ove_score.argmax(1)), ove_score.max(1))) # [(cluster_id1, cluster_id2), ove_score]\n",
    "            ove_clusters_pair_id_w = [j for j in ove_clusters_pair_id_w if j[1] >= self.merge_threshold]                  # keep those with overlap score >= threshold\n",
    "            \n",
    "            if len(ove_clusters_pair_id_w) < 1:\n",
    "                if_merge = False\n",
    "            else:\n",
    "                ## Index of clusters with similarity highr than a threshold\n",
    "                ove_clusters_id = list(set([j for k in [i[0] for i in ove_clusters_pair_id_w] for j in k]))\n",
    "                \n",
    "                ## Sorted by overlap score\n",
    "                ove_clusters_pair_id_w = sorted(ove_clusters_pair_id_w, key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                ## Combind cluster id, overlap score, and overlap size\n",
    "                ove_clusters_pair_id_w_size = [ove_clusters_pair_id_w[idx] + \n",
    "                                               ( len(set(Cs[ove_clusters_pair_id_w[idx][0][0]] + \n",
    "                                                         Cs[ove_clusters_pair_id_w[idx][0][1]])), ) \n",
    "                                               for idx in range(len(ove_clusters_pair_id_w))] # [(cluster_id1, cluster_id2), ove_score, ove_size]\n",
    "                \n",
    "                ## filter by max size\n",
    "                ove_clusters_pair_id_w_size = [j for j in ove_clusters_pair_id_w_size if j[2] <= self.max_complex_size]\n",
    "                \n",
    "                if len(ove_clusters_pair_id_w_size) < 1:\n",
    "                    if_merge = False\n",
    "                else:\n",
    "                    ## combind pairs to set\n",
    "                    ove_clusters_pair_id_w_size_set = [[j for j in ove_clusters_pair_id_w_size if k in j[0]] for k in ove_clusters_id]\n",
    "                    ove_clusters_pair_id_w_size_set = [j for j in ove_clusters_pair_id_w_size_set if len(j) > 0]\n",
    "                    \n",
    "                    if len(ove_clusters_pair_id_w_size_set) < 1:\n",
    "                        if_merge = False\n",
    "                    else:\n",
    "                        ## Collect all the id of clusters that need to merge\n",
    "                        ove_clusters_id_set = list(set( [tuple(sorted( set(chain.from_iterable( [j[0] for j in ove_clusters_pair_id_w_size_set[k]] )) )) \n",
    "                                                         for k in range(len(ove_clusters_pair_id_w_size_set))] ))\n",
    "                        \n",
    "                        ## Remove subset\n",
    "                        ove_clusters_id_set_sub = self.remove_subset(ove_clusters_id_set)\n",
    "                        \n",
    "                        ## Index of clusters that will be merged / not be merged\n",
    "                        ove_clusters_id = list(set([k for j in ove_clusters_id_set_sub for k in j]))\n",
    "                        non_ove_clusters_id = list(set(range(len(Cs)))^set(ove_clusters_id))\n",
    "                        \n",
    "                        Cs = [tuple(sorted( set(chain.from_iterable([Cs[j] for j in ove_clusters_id_set_sub[k]])) )) \n",
    "                              for k in range(len(ove_clusters_id_set_sub))] + [Cs[j] for j in non_ove_clusters_id]\n",
    "                        \n",
    "                        small_Cs = [j for j in Cs if (len(j) >= self.min_complex_size)&(len(j) <= self.max_complex_size)]\n",
    "                        large_Cs = [j for j in Cs if len(j) > self.max_complex_size]\n",
    "                        \n",
    "                        if len(large_Cs) > 0 and self.merge_num <= self.max_merge_num:\n",
    "                            keep_idx = np.array(list(set(chain.from_iterable([j for j in large_Cs]))))\n",
    "                            small_Cs = small_Cs + self.extract_clusters(A, keep_idx=keep_idx)\n",
    "                            small_Cs = self.remove_subset(small_Cs)\n",
    "                        else:\n",
    "                            if_merge = False\n",
    "                        \n",
    "                        Cs = small_Cs\n",
    "                        self.merge_num += 1\n",
    "        \n",
    "        self.merge_num = 0\n",
    "        return Cs\n",
    "    \n",
    "    def run(self, e_idx, e_w=None, dim=None, diag=None):\n",
    "        if dim is None:\n",
    "            print('Dim must specified')\n",
    "            return\n",
    "        else:\n",
    "            A = self.create_edge_adj(e_idx, e_w=e_w, dim=dim, diag=diag)\n",
    "            Cs = self.extract_clusters(A)\n",
    "            Cs = self.merge_clusters(Cs, A)\n",
    "            Cs = self.remove_subset(Cs)\n",
    "            Cs = list(set(Cs))\n",
    "            return Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c852149-7fcc-4016-915f-bffaca347710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13d44b3c-b157-4a42-8c02-fc7f78c7c979",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abc5a2-6101-499c-b478-c13bf266e9be",
   "metadata": {},
   "source": [
    "### Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f13d84-7b87-4b09-9632-a498d606e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = list(pyreadr.read_r(comp_path).values())[0]\n",
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3abac648-6037-4d06-aaea-f382edfa2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = list(gs.groupby('ComplexName')['Gene_name'].apply(list).values)\n",
    "print(len(gs))\n",
    "print(*gs[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b370ba-91cd-4511-af7e-4704193ccf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(dict_path, 'rb') as f:\n",
    "    name_idx_dict = pickle.load(f)\n",
    "print(len(name_idx_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "877f501f-df20-44b3-a1b1-53c4c848a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter gs by exp name\n",
    "gs_sub = [[i for i in gs[j] if i in name_idx_dict.keys()] for j in range(len(gs))]\n",
    "print(len(gs_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ac0b2b3-b782-41e0-b18b-9ed48012018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### match name by idx\n",
    "complexes = [tuple([name_idx_dict[i] for i in gs_sub[j]]) for j in range(len(gs_sub))]\n",
    "print(len(complexes))\n",
    "print(*complexes[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "635e9da9-4be0-4345-a8e8-2d13a415e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter by complex size\n",
    "complexes = [i for i in complexes if len(i) > 2]\n",
    "print(len(complexes))\n",
    "print(*complexes[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ae24de-ebdd-460c-a044-473632754616",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156bbabe-1ee9-4683-9892-957da6e58b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcl = MCL_Cluster(merge_threshold=0.25, min_complex_size=3, max_complex_size=100, split_depth=3)\n",
    "clusters = mcl.run(all_edge, e_w=all_pred.detach().numpy(), dim=epf.shape[0], diag=1.0)\n",
    "overlap_score = mcl.get_ove_score(clusters, complexes)\n",
    "\n",
    "best_matched_cluster = overlap_score.argmax(0) # best matched cluster for each complex\n",
    "best_matched_complex = overlap_score.argmax(1) # best matched complex for each cluster\n",
    "best_matched_cluster_score = overlap_score.max(0) # overlap perc between best matched cluster and each complex\n",
    "best_matched_complex_score = overlap_score.max(1) # overlap perc between best matched complex and each cluster\n",
    "\n",
    "best_matched_cluster_info = [tuple([i, best_matched_cluster[i], best_matched_cluster_score[i]]) for i in range(len(best_matched_cluster))]\n",
    "# 0:complex id, 1:best cluster id, 2:overlap perc\n",
    "best_matched_complex_info = [tuple([i, best_matched_complex[i], best_matched_complex_score[i]]) for i in range(len(best_matched_complex))]\n",
    "# 0:cluster id, 1:best complex id, 2:overlap perc\n",
    "best_matched_cluster_info = sorted(best_matched_cluster_info, key=lambda x: x[2], reverse=True)\n",
    "best_matched_complex_info = sorted(best_matched_complex_info, key=lambda x: x[2], reverse=True)\n",
    "cluster_cluster_score = mcl.get_ove_score(clusters, clusters)\n",
    "\n",
    "print(len(clusters), min([len(i) for i in clusters]), max([len(i) for i in clusters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077c53f4-25f9-4155-9739-05267e5dd34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5720823d-eb29-4cc7-9837-24e6098f1746",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2b1c636-0302-465c-9432-64dcf356b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = './Cluster'\n",
    "if not os.path.exists(out_path):\n",
    "      os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "696dfbc0-b31a-49da-b4e4-df071d808b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save clusters\n",
    "with open('./Cluster/' + '_'.join([exp_cond, 'clusters.txt']), 'w') as output:\n",
    "    for line in clusters:\n",
    "        s = \" \".join(map(str, line))\n",
    "        output.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a7aeaef-5fba-43dc-90b2-7e847e05fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save cluster info\n",
    "np.save('./Cluster/' + '_'.join([exp_cond, 'overlap_score.npy']), overlap_score)\n",
    "np.save('./Cluster/' + '_'.join([exp_cond, 'best_matched_cluster_info.npy']), best_matched_cluster_info)\n",
    "np.save('./Cluster/' + '_'.join([exp_cond, 'best_matched_complex_info.npy']), best_matched_complex_info)\n",
    "np.save('./Cluster/' + '_'.join([exp_cond, 'cluster_cluster_score.npy']), cluster_cluster_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821581a0-8d0f-4500-8bc1-01ad0b51c170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7190631b-ee66-438d-9cdf-9cf2e39408b3",
   "metadata": {},
   "source": [
    "## Load ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7210a02-6ed7-4db8-b3dc-753dd81a535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Cluster/' + '_'.join([exp_cond, 'clusters.txt']),'r') as fh:\n",
    "    clusters = []\n",
    "    for oneline in fh:\n",
    "        oneline = oneline.strip()\n",
    "        clusters.append(oneline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f447cd-ceb4-45e1-baf8-dfe04ee49d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e835ef-51ea-4891-b647-19a6459c7eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd0c8a-da82-485d-addd-c2dbe8e27b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
